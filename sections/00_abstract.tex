High Power Laser (HPL) systems operate in the femtosecond regime—one of the shortest timescales achievable in experimental physics. 
HPL systems are instrumental in high-energy physics, leveraging ultra-short impulse durations to yield extremely high intensities, which are essential for both practical applications and theoretical advancements in light-matter interactions. Traditionally, the parameters regulating HPL optical performance are tuned manually by human experts, or optimized by using black-box methods that can be computationally demanding. Critically, black box methods rely on stationarity assumptions overlooking complex dynamics in high-energy physics and day-to-day changes in real-world experimental settings, and thus need to be often restarted. Deep Reinforcement Learning (DRL) offers a promising alternative by enabling sequential decision making in non-static settings. This work explores the 
feasibility of applying DRL to HPL systems under two practical constraints: (1) the controller must rely solely on image observations from diagnostic devices, and (2) it must remain effective when the underlying experimental conditions vary. We present initial empirical results—benchmarked against a representative Bayesian–Optimisation baseline in simulation—indicating that DRL can reduce unsafe exploration and retain roughly $90\%$ of the target intensity across a range of simulated operating points.