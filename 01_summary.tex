High Power Laser (HPL) systems operate in the femtosecond regime---one of the shortest timescales achievable in experimental physics.
These systems are instrumental in high-energy physics, as ultra-short pulses yield extremely high intensities, which play out to be essential in both practical applications and theoretical advancements in light-matter interactions.
Traditionally, the parameters regulating HPL optical performance are optimized using black-box numerical methods such as Evolution Strategies (ES), and Bayesian Optimization (BO). While effective, black-box methods are computationally demanding and rely on stationarity assumptions overlooking transient and complex system dynamics in HPL systems. Moreover, their safe implementation on real-world hardware is challenging, as erratic exploration of the parameter space can compromise system safety.
Model-free Deep Reinforcement Learning (DRL) offers a promising alternative by enabling sequential decision making in non-static settings. 
This work investigates the safe application of DRL to HPL systems, and extends current research by (1) learning a control policy directly from pixels, using images typically available in experimental settings and (2) addressing the need for generalization across diverse dynamics, tackling the non-stationarity of the environment.
We evaluate our method in simulation across various dynamic configurations and observe that DRL effectively enables cross-domain adaptability by transferring knowledge across conditions---eliminating the need to restart ES/BO whenever there are fluctuations in the environment dynamics.
Our contributions represent a significant step towards real-world applications of DRL to HPL systems, introducing the RL community to the task of controlling complex non-linear physical systems used to ignite nuclear fusion and accelerate charged particles.